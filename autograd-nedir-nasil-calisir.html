<!DOCTYPE html>
<html lang="en">
<head>
          
        <title>DenDen - Autograd Nedir, Nasıl Çalışır?</title>
        
        <meta charset="utf-8" />
        <meta name="generator" content="Pelican" />
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        
        

        <link href="/theme/css/custom.css" rel="stylesheet">
        




    <meta name="tags" content="Machine Learning" />
    <meta name="tags" content="Differential Programming" />
    <meta name="tags" content="Autograd" />

</head>

<body>
      <header>
        <nav>

            <h4 style="margin-right: 0.35em;">
              <a href="/">DenDen</a>
            </h4>
            
            <!-- Vertical bar -->
            <span style="margin-right: 0.35em;">|</span>

            
                

          
                <a href="/category/differential-programming.html"  aria-current="page" >Differential Programming</a>
          
            <a href="/pages/about.html"> About </a>

        </nav>
      </header>
      
      <main>
<article>
  <header>
    <h2>
      <a href="/autograd-nedir-nasil-calisir.html" rel="bookmark"
         title="Permalink to Autograd Nedir, Nasıl Çalışır?">Autograd Nedir, Nasıl Çalışır?
      </a>
    </h2>

    

    <div class="small-text">
      <p>Published: <time datetime="2024-01-27T00:00:00+03:00">
        Sat 27 January 2024
      </time></p>

      
      <!-- 
      <p>
          Category: <a href="/category/differential-programming.html">Differential Programming</a>
      </p>
 
      -->

      <p>
          Tags:
              <a href="/tag/machine-learning.html">Machine Learning</a>
              <a href="/tag/differential-programming.html">Differential Programming</a>
              <a href="/tag/autograd.html">Autograd</a>
      </p>
    </div>

  </header>

  <script type="module"> import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs'; mermaid.initialize({ startOnLoad: true }); </script>

<h3>AutoGrad Nedir?</h3>
<p>Autograd makine öğrenme kütüphanelerinde optimize edilmeye çalışılan fonksiyon için otomatik olarak türev alma sistemlerine verilen addır. Bu sistemlerde genellikle işlemler bir ağaç yapısı ile gösterilir. "Backpropagation" matematiğinin temelini autograd sistemleri oluşturur.</p>
<h3>Autograd Ağaç Yapısı</h3>
<p>Autograd ağaç yapısı, bilgisayar bilmindeki ağaç yapısı ile aynıdır. Burada düğüm adında, kutu içinde bulunan yazılar oklu yada oksuz bağlantılar ile bağlanır.</p>
<p>Autograd için örnek bir ağaç yapısı şu şekildedir.</p>
<pre class="mermaid">
graph LR
    A["5.0"]
    B["8.0"]
    C["13.0"]

    A --> +;
    B --> +;
    + --> C;
    2.0 --> *;
    C --> *;
    * --> 26.0;
</pre>

<p>Bu ağaç yapısı <span class="math">\((5.0 + 8.0) \times 2.0 = 26.0\)</span> işleminin gösterimidir. </p>
<h3>Peki Ağaçlar üzerinde türev nasıl yapılır?</h3>
<p>Yukarıdaki grafdaki değerleri bir değişken olarak alırsak:</p>
<pre class="mermaid">
graph LR
    A["a"]
    B["b"]
    C["c"]

    A --> +;
    B --> +;
    + --> C;
    d --> *;
    C --> *;
    * --> e;
</pre>

<p>Şeklinde düşünebilirz. Buradaki çarpma işlemi için türev hesapları ise şöyle olacak.</p>
<div class="math">$$
e=d \times c \quad \frac{de}{de} = 1.0 \quad \frac{de}{dc} = d \quad \frac{de}{dd} = c
$$</div>
<p>Burada dikkati çeken iki şey var. Biri çarpmanın türevinin iki değişken için diğer değişkenin değeri olması, diğeri ise kendisnin kendisiyle türevi 1.0 olması.</p>
<p>Şimdi asıl sıkınta sonuç değeri olan <code>e</code> değerinden uzak olan işlemlerin hesabı için örneğin <span class="math">\(\frac{de}{da}\)</span> nasıl hesaplanacak. Bu basit bir örnek bunda hesapladık diyelim bunu algoritmik bir şekilde milyonlarca parametresi olan fonksiyonlarla nasıl yapacağız.</p>
<p>Burada bizi zincir kuralı kurtarıyor zincir kuralına göre <span class="math">\(\frac{de}{da} = \frac{de}{dc} \times \frac{dc}{da}\)</span> zaten biz <span class="math">\(\frac{de}{dc}\)</span> hesaplamıştık yani sadece <span class="math">\(\frac{dc}{da}\)</span> hesaplamamız yeterli.</p>
<p>Zincir kuralı sayesinde işlemleri oluşturan her değer için lokal gradyanı hesaplamak yeterli. Global gradyanı elde etmek isdeğimiz de sadece işlemin verdiği sonuçun gradyanı ile çarpıcağız.</p>
<p>Toplama için kurallarda bu şekilde:</p>
<div class="math">$$
c=a + b \quad \frac{dc}{dc} = 1.0 \quad \frac{dc}{da} = 1.0 \quad \frac{dc}{db} = 1.0
$$</div>
<p>Hepsi <span class="math">\(1.0\)</span> oldu, bu değerlerin bir üst seviyedeki gradyan ile çarpılcağını düşünürsek toplama işleminde gradyanlar aynı şekilde aktarıldığını düşünebiliriz.</p>
<h3>Örnek Implementasyon</h3>
<p>Şimdi burada javascriptte basit bir autograd sistemi yapalım. İlk olarak sınıf tanımlarını yapıyoruz.</p>
<div class="highlight"><pre><span></span><code><span class="kd">class</span><span class="w"> </span><span class="nx">Scalar</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="kr">constructor</span><span class="p">(</span><span class="nx">value</span><span class="p">,</span><span class="w"> </span><span class="nx">children</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">[]){</span>
<span class="w">    </span><span class="k">this</span><span class="p">.</span><span class="nx">value</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nx">value</span>
<span class="w">    </span><span class="k">this</span><span class="p">.</span><span class="nx">grad</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mf">0.0</span>
<span class="w">    </span><span class="k">this</span><span class="p">.</span><span class="nx">__backwards</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">()</span><span class="w"> </span><span class="p">=&gt;</span><span class="w"> </span><span class="kc">null</span>
<span class="w">    </span><span class="k">this</span><span class="p">.</span><span class="nx">children</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nx">children</span>
<span class="w">  </span><span class="p">}</span>
<span class="p">}</span>
</code></pre></div>

<p>Burada <code>value</code> tutulan değer <code>grad</code> gradyan hesabının sonucu <code>__backwards</code> gradyanı hesaplayacak fonksiyon ve <code>children</code> bu değeri almak için hesaplanan diğerler örneğin <code>a = b + c</code> deki <code>a</code> değerinin children değerleri <code>[b, c]</code> olur.</p>
<div class="highlight"><pre><span></span><code><span class="w">  </span><span class="nx">add_scalar</span><span class="p">(</span><span class="nx">other</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="kd">let</span><span class="w"> </span><span class="nx">out</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="ow">new</span><span class="w"> </span><span class="nx">Scalar</span><span class="p">(</span><span class="k">this</span><span class="p">.</span><span class="nx">value</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="nx">other</span><span class="p">.</span><span class="nx">value</span><span class="p">,</span><span class="w"> </span><span class="p">[</span><span class="k">this</span><span class="p">,</span><span class="w"> </span><span class="nx">other</span><span class="p">])</span>

<span class="w">    </span><span class="c1">// a = b + c,  da/db = 1.0,  da/dc = 1.0</span>
<span class="w">    </span><span class="k">this</span><span class="p">.</span><span class="nx">__backwards</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">()</span><span class="w"> </span><span class="p">=&gt;</span><span class="w"> </span><span class="mf">1.0</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="nx">out</span><span class="p">.</span><span class="nx">grad</span>
<span class="w">    </span><span class="nx">other</span><span class="p">.</span><span class="nx">__backwards</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">()</span><span class="w"> </span><span class="p">=&gt;</span><span class="w"> </span><span class="mf">1.0</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="nx">out</span><span class="p">.</span><span class="nx">grad</span>

<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="nx">out</span>
<span class="w">  </span><span class="p">}</span>
</code></pre></div>

<p>Bu toplama işleminin tanımı burada out diye yeni bir value değeri oluşturup dönderiyoruz. Bunu yaparkende <code>children</code> kısmınıda kullandığımız değerler olarak yapıyoruz. Bir üst kısımda toplamanın türevinde bahsetmiştim burada türevler 1.0 olduğundan sonucun gradı neyse bu değerlerinde gradı öyle olucak.</p>
<div class="highlight"><pre><span></span><code><span class="w">  </span><span class="nx">mult_scalar</span><span class="p">(</span><span class="nx">other</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="kd">let</span><span class="w"> </span><span class="nx">out</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="ow">new</span><span class="w"> </span><span class="nx">Scalar</span><span class="p">(</span><span class="k">this</span><span class="p">.</span><span class="nx">value</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="nx">other</span><span class="p">.</span><span class="nx">value</span><span class="p">,</span><span class="w"> </span><span class="p">[</span><span class="k">this</span><span class="p">,</span><span class="w"> </span><span class="nx">other</span><span class="p">])</span>

<span class="w">    </span><span class="c1">// a = b * c,  da/db = c,  da/dc = b</span>
<span class="w">    </span><span class="k">this</span><span class="p">.</span><span class="nx">__backwards</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">()</span><span class="w"> </span><span class="p">=&gt;</span><span class="w"> </span><span class="nx">other</span><span class="p">.</span><span class="nx">value</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="nx">out</span><span class="p">.</span><span class="nx">grad</span>
<span class="w">    </span><span class="nx">other</span><span class="p">.</span><span class="nx">__backwards</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">()</span><span class="w"> </span><span class="p">=&gt;</span><span class="w"> </span><span class="k">this</span><span class="p">.</span><span class="nx">value</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="nx">out</span><span class="p">.</span><span class="nx">grad</span>

<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="nx">out</span>
<span class="w">  </span><span class="p">}</span>
</code></pre></div>

<p>Buda çarpmanın tanımı tıplama ile aynı tek fark hesaplama işleminin çarpma olması ve gradyanların farklı hesaplanması.</p>
<p>Şimdi basti bir test yapalım. İlk olarak kullanacğımız değerleri tanımlıyoruz. Bunlar bizim gradyanın hesaplayacğımız değerler olacak.</p>
<div class="highlight"><pre><span></span><code><span class="kd">let</span><span class="w"> </span><span class="nx">a</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="ow">new</span><span class="w"> </span><span class="nx">Scalar</span><span class="p">(</span><span class="mf">5.0</span><span class="p">)</span>
<span class="kd">let</span><span class="w"> </span><span class="nx">b</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="ow">new</span><span class="w"> </span><span class="nx">Scalar</span><span class="p">(</span><span class="mf">3.0</span><span class="p">)</span>
<span class="kd">let</span><span class="w"> </span><span class="nx">c</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="ow">new</span><span class="w"> </span><span class="nx">Scalar</span><span class="p">(</span><span class="mf">1.5</span><span class="p">)</span>
</code></pre></div>

<p>Şimdi aşşağıdaki grafdaki işlemi <code>Value</code> sınıfnı kullanarak göstereceğiz.</p>
<pre class="mermaid">
flowchart LR
    5.0 --> +;
    3.0 --> +;
    + --> 8.0;
    8.0 --> *;
    1.5 --> *;
    * --> 12.0;
</pre>

<p>Bu işlemi <code>Value</code> sınıfnın metodlarını kullanarak yapıyoruz.</p>
<div class="highlight"><pre><span></span><code><span class="kd">let</span><span class="w"> </span><span class="nx">ab</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nx">a</span><span class="p">.</span><span class="nx">add_scalar</span><span class="p">(</span><span class="nx">b</span><span class="p">)</span>
<span class="kd">let</span><span class="w"> </span><span class="nx">ab_c</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nx">ab</span><span class="p">.</span><span class="nx">mult_scalar</span><span class="p">(</span><span class="nx">c</span><span class="p">)</span>
</code></pre></div>

<p>İşlemler bittikten sonra <code>ab_c.value</code> değerinin <code>12.0</code> olduğunu görebiliriz. Şimdi backwards propagation algoritmasını denemek için sondan başa (graf için sağdan sol) doğru gradyan hesabı yapıyoruz.</p>
<div class="highlight"><pre><span></span><code><span class="nx">ab_c</span><span class="p">.</span><span class="nx">grad</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mf">1.0</span>
<span class="nx">ab</span><span class="p">.</span><span class="nx">grad</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nx">ab</span><span class="p">.</span><span class="nx">__backwards</span><span class="p">()</span>
<span class="nx">c</span><span class="p">.</span><span class="nx">grad</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nx">c</span><span class="p">.</span><span class="nx">__backwards</span><span class="p">()</span>
<span class="nx">a</span><span class="p">.</span><span class="nx">grad</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nx">a</span><span class="p">.</span><span class="nx">__backwards</span><span class="p">()</span>
<span class="nx">b</span><span class="p">.</span><span class="nx">grad</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nx">b</span><span class="p">.</span><span class="nx">__backwards</span><span class="p">()</span>
</code></pre></div>

<p>Burada <code>ab_c</code> değerinin 1.0 olması zaten bu değer için türev aldığımızından. Yani kendine türevi 1.0 olduğundan.</p>
<p>Gradyan hesaplamaları bittikten sonra bu değerlere baktığımızda hepsinin doğru olduğunu göreceğiz.</p>
<div class="highlight"><pre><span></span><code><span class="nx">console</span><span class="p">.</span><span class="nx">log</span><span class="p">(</span><span class="nx">ab_c</span><span class="p">.</span><span class="nx">grad</span><span class="p">)</span>
<span class="c1">// 1.0</span>
<span class="nx">console</span><span class="p">.</span><span class="nx">log</span><span class="p">(</span><span class="nx">ab</span><span class="p">.</span><span class="nx">grad</span><span class="p">)</span>
<span class="c1">// 1.5</span>
<span class="nx">console</span><span class="p">.</span><span class="nx">log</span><span class="p">(</span><span class="nx">c</span><span class="p">.</span><span class="nx">grad</span><span class="p">)</span>
<span class="c1">// 8</span>
</code></pre></div>

<p>Şimdide tüm işlemlere bir araya toplamak için "backwards propagation" algoritmasını kullanacağız. İlk olarak elimizdeki değerleri <a href="https://en.wikipedia.org/wiki/Topological_sorting">topolojik olarak</a> sıralamamız gerekiyor.  Bunun sebebi her bir gradyan hesabının sonuç değerdeki gradyana bağlı olması yani gradyan hesabına sondan başa doğru bir şekilde yapmamız gerekiyor.</p>
<div class="highlight"><pre><span></span><code><span class="w">  </span><span class="nx">backward</span><span class="p">()</span><span class="w"> </span><span class="p">{</span>

<span class="w">    </span><span class="kd">let</span><span class="w"> </span><span class="nx">visited</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">[]</span>
<span class="w">    </span><span class="kd">let</span><span class="w"> </span><span class="nx">topo</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">[]</span>
<span class="w">    </span><span class="kd">function</span><span class="w"> </span><span class="nx">recur_topo_sort</span><span class="p">(</span><span class="nx">node</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">      </span><span class="c1">// Check visited</span>
<span class="w">      </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="nx">visited</span><span class="p">.</span><span class="nx">includes</span><span class="p">(</span><span class="nx">node</span><span class="p">))</span><span class="w"> </span><span class="k">return</span><span class="p">;</span>
<span class="w">      </span><span class="nx">visited</span><span class="p">.</span><span class="nx">push</span><span class="p">(</span><span class="nx">node</span><span class="p">)</span>
<span class="w">      </span><span class="c1">// Recurse</span>
<span class="w">      </span><span class="nx">node</span><span class="p">.</span><span class="nx">children</span><span class="p">.</span><span class="nx">forEach</span><span class="p">((</span><span class="nx">child</span><span class="p">)</span><span class="w"> </span><span class="p">=&gt;</span><span class="w"> </span><span class="p">{</span><span class="nx">recur_topo_sort</span><span class="p">(</span><span class="nx">child</span><span class="p">)})</span>
<span class="w">      </span><span class="c1">// Add to list</span>
<span class="w">      </span><span class="nx">topo</span><span class="p">.</span><span class="nx">push</span><span class="p">(</span><span class="nx">node</span><span class="p">)</span>
<span class="w">    </span><span class="p">}</span>

<span class="w">    </span><span class="nx">recur_topo_sort</span><span class="p">(</span><span class="k">this</span><span class="p">)</span>

<span class="w">        </span><span class="k">this</span><span class="p">.</span><span class="nx">grad</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mf">1.0</span>

<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kd">let</span><span class="w"> </span><span class="nx">index</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nx">topo</span><span class="p">.</span><span class="nx">length</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="mf">2</span><span class="p">;</span><span class="w"> </span><span class="nx">index</span><span class="w"> </span><span class="o">&gt;=</span><span class="w"> </span><span class="mf">0</span><span class="p">;</span><span class="w"> </span><span class="nx">index</span><span class="o">--</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">      </span><span class="kd">let</span><span class="w"> </span><span class="nx">node</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nx">topo</span><span class="p">[</span><span class="nx">index</span><span class="p">];</span>
<span class="w">        </span><span class="nx">node</span><span class="p">.</span><span class="nx">grad</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="nx">node</span><span class="p">.</span><span class="nx">__backwards</span><span class="p">()</span>
<span class="w">    </span><span class="p">}</span>

<span class="w">  </span><span class="p">}</span>
</code></pre></div>

<p>Artık tüm gradyanları hesaplamak için <code>backward</code> fonksiyonunu kullanabiliyoruz.</p>
<div class="highlight"><pre><span></span><code><span class="kd">let</span><span class="w"> </span><span class="nx">a</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="ow">new</span><span class="w"> </span><span class="nx">Scalar</span><span class="p">(</span><span class="mf">5.0</span><span class="p">)</span>
<span class="kd">let</span><span class="w"> </span><span class="nx">b</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="ow">new</span><span class="w"> </span><span class="nx">Scalar</span><span class="p">(</span><span class="mf">3.0</span><span class="p">)</span>
<span class="kd">let</span><span class="w"> </span><span class="nx">c</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="ow">new</span><span class="w"> </span><span class="nx">Scalar</span><span class="p">(</span><span class="mf">1.5</span><span class="p">)</span>

<span class="kd">let</span><span class="w"> </span><span class="nx">ab</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nx">a</span><span class="p">.</span><span class="nx">add_scalar</span><span class="p">(</span><span class="nx">b</span><span class="p">)</span>
<span class="kd">let</span><span class="w"> </span><span class="nx">ab_c</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nx">ab</span><span class="p">.</span><span class="nx">mult_scalar</span><span class="p">(</span><span class="nx">c</span><span class="p">)</span>

<span class="nx">ab_c</span><span class="p">.</span><span class="nx">backward</span><span class="p">()</span>
</code></pre></div>

<h3>Pytorch Autograd</h3>
<p>Şimdi aynı işlemleri pytorch kütüphanesi kullanarak <code>perceptron</code> hesabının nasıl yapılacağına bakalım.</p>
<div class="highlight"><pre><span></span><code><span class="kn">import</span> <span class="nn">torch</span>

<span class="k">def</span> <span class="nf">scalar_tensor</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="n">t</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">([</span><span class="n">x</span><span class="p">])</span><span class="o">.</span><span class="n">double</span><span class="p">()</span>
    <span class="n">t</span><span class="o">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="kc">True</span>  <span class="c1"># Yaprak düğümler için gerekli</span>
    <span class="k">return</span> <span class="n">t</span>
</code></pre></div>

<p>Bu torchdaki tensör veri yapılarının yukarıdaki <code>Value</code> veri yapısına benzemesi için yardımcı fonksiyon.</p>
<div class="highlight"><pre><span></span><code><span class="n">x1</span> <span class="o">=</span> <span class="n">scalar_tensor</span><span class="p">(</span><span class="o">-</span><span class="mf">3.0</span><span class="p">)</span>
<span class="n">x2</span> <span class="o">=</span> <span class="n">scalar_tensor</span><span class="p">(</span><span class="mf">0.5</span><span class="p">)</span>

<span class="n">w1</span> <span class="o">=</span> <span class="n">scalar_tensor</span><span class="p">(</span><span class="o">-</span><span class="mf">1.0</span><span class="p">)</span>
<span class="n">w2</span> <span class="o">=</span> <span class="n">scalar_tensor</span><span class="p">(</span><span class="mf">2.5</span><span class="p">)</span>

<span class="n">b</span> <span class="o">=</span> <span class="n">scalar_tensor</span><span class="p">(</span><span class="mf">6.75</span><span class="p">)</span>
</code></pre></div>

<p>Perceptron değişkenleri tanımlandıktan sonra hesabı için:</p>
<div class="highlight"><pre><span></span><code><span class="n">n</span> <span class="o">=</span> <span class="n">x1</span><span class="o">*</span><span class="n">w1</span> <span class="o">+</span> <span class="n">x2</span><span class="o">*</span><span class="n">w2</span> <span class="o">+</span> <span class="n">b</span>
<span class="n">o</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">o</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
<span class="mf">0.9999999994421064</span>
</code></pre></div>

<p>"Forward pass" yaptıktan sonra  şimdi backpropagation hesabı yapalım.</p>
<div class="highlight"><pre><span></span><code><span class="n">o</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>  <span class="c1"># grad değerleri hesapla</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">x1</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
<span class="o">-</span><span class="mf">1.115787240379973e-09</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">x2</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
<span class="mf">2.7894681009499322e-09</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">w1</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
<span class="o">-</span><span class="mf">3.3473617211399187e-09</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">w2</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
<span class="mf">5.578936201899864e-10</span>
</code></pre></div>

<h3>Sonuç</h3>
<p>Bu yazıda autograd sistemlerinin çok basit bir presnipe dayandığını, nasıl çalıştığını ve örneğini gördük.</p>
<p>Burada okura bir egsersiz olarak "Çarpma işleminin türevinde neden gradyanların diğer değişkenin değeri olduğunun sezgisel açıklaması?" bırakıyorum.</p>
<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width < 768) ? "left" : align;
        indent = (screen.width < 768) ? "0em" : indent;
        linebreak = (screen.width < 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script>

  <!-- <footer>
  </footer> -->

  </article>
      </main>
      
      <footer>
              <address>
              Powered by <a rel="nofollow" href="https://getpelican.com/">Pelican</a>
              </address>
      </footer>
</body>
</html>